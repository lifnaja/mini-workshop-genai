import feedparser
import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

def fetch_cnbc_rss():
    rss_url = "https://www.cnbc.com/id/100003114/device/rss/rss.html"
    feed = feedparser.parse(rss_url)

    news_items = []
    for entry in feed.entries[:10]:  # ‡∏à‡∏≥‡∏Å‡∏±‡∏î 10 ‡∏Ç‡πà‡∏≤‡∏ß‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏£‡πá‡∏ß (‡πÄ‡∏≠‡∏≤‡∏≠‡∏≠‡∏Å‡∏ñ‡πâ‡∏≤‡∏à‡∏∞‡∏î‡∏∂‡∏á‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î)
        news_items.append({
            'title': entry.title,
            'link': entry.link,
            'published': entry.published
        })
    return news_items

def scrape_article_content(url):
    try:
        headers = {"User-Agent": "Mozilla/5.0"}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")

        # CNBC ‡πÉ‡∏ä‡πâ tag ‡∏ô‡∏µ‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏Ç‡πà‡∏≤‡∏ß
        paragraphs = soup.select('div.ArticleBody-articleBody p')
        content = "\n".join(p.get_text() for p in paragraphs if p.get_text(strip=True))
        return content
    except Exception as e:
        print(f"‚ùå Error fetching {url}: {e}")
        return None

def get_latest_news_with_content():
    news_items = fetch_cnbc_rss()
    for news in news_items:
        print(f"üì• Fetching: {news['title']}")
        news['content'] = scrape_article_content(news['link'])
        time.sleep(1)  # ‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÇ‡∏î‡∏ô block
    return pd.DataFrame(news_items)

# ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô
if __name__ == "__main__":
    df = get_latest_news_with_content()
    pd.set_option('display.max_colwidth', 200)
    print(df[['title', 'published', 'content']].head())
